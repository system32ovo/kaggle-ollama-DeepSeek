{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:48:29.884253Z",
     "iopub.status.busy": "2025-02-05T01:48:29.883948Z",
     "iopub.status.idle": "2025-02-05T01:48:56.261026Z",
     "shell.execute_reply": "2025-02-05T01:48:56.260201Z",
     "shell.execute_reply.started": "2025-02-05T01:48:29.884230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#貌似Miniconda在安装时会自动下载安装Python3.12.8版本open-webui需要python>=3.11,而在kaggle里面安装python并配置环境变量又不怎么容易所以就直接使用Miniconda\n",
    "#安装脚本来安装python3.12.8并配置环境变量\n",
    "#kaggle截止2025/2/5使用的python版本依然是3.10.9,安装python3.12.8\n",
    "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-05T01:48:56.262204Z",
     "iopub.status.busy": "2025-02-05T01:48:56.261939Z",
     "iopub.status.idle": "2025-02-05T01:54:55.754664Z",
     "shell.execute_reply": "2025-02-05T01:54:55.753721Z",
     "shell.execute_reply.started": "2025-02-05T01:48:56.262170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install open-webui\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:59:43.088193Z",
     "iopub.status.busy": "2025-02-05T01:59:43.087805Z",
     "iopub.status.idle": "2025-02-05T02:00:49.295567Z",
     "shell.execute_reply": "2025-02-05T02:00:49.294653Z",
     "shell.execute_reply.started": "2025-02-05T01:59:43.088162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import socket\n",
    "from colorama import Fore, Style\n",
    "import urllib.request\n",
    "\n",
    "model_name = \"deepseek-r1:14b\"\n",
    "\n",
    "def read_stream(pipe, handler):\n",
    "    \"\"\"Read stream from pipe and apply the handler function.\"\"\"\n",
    "    try:\n",
    "        for line in iter(pipe.readline, ''):\n",
    "            if line:\n",
    "                handler(line.strip())\n",
    "    except ValueError:\n",
    "        # 忽略管道关闭时可能抛出的异常\n",
    "        pass\n",
    "    finally:\n",
    "        pipe.close()\n",
    "\n",
    "def run_ollama_task():\n",
    "    print(\"Ollama task started\")\n",
    "    # 使用 subprocess.Popen 启动 ollama serve 命令\n",
    "    process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    time.sleep(5)\n",
    "    pull_model = subprocess.Popen([\"ollama\", \"pull\", model_name], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # 创建线程来读取输出和错误信息\n",
    "    t_out = threading.Thread(target=read_stream, args=(pull_model.stdout, print))\n",
    "    t_err = threading.Thread(target=read_stream, args=(pull_model.stderr, lambda x: print(Fore.RED + x + Style.RESET_ALL)))\n",
    "    t_out.daemon = True\n",
    "    t_err.daemon = True\n",
    "    t_out.start()\n",
    "    t_err.start()\n",
    "\n",
    "    pull_model.wait()  # 等待模型拉取完成\n",
    "    if pull_model.returncode == 0:\n",
    "        print(Fore.GREEN + \"Model download completed\" + Style.RESET_ALL)\n",
    "    else:\n",
    "        print(Fore.RED + \"Model download failed\" + Style.RESET_ALL)\n",
    "\n",
    "    list_models = subprocess.Popen([\"ollama\", \"list\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    read_stream(list_models.stdout, print)  # 实时捕获并处理列表命令的输出\n",
    "    list_models.wait()\n",
    "    print(Fore.GREEN + \"Ollama task is running in the background\" + Style.RESET_ALL)\n",
    "\n",
    "def run_open_webui():\n",
    "    print(\"open-webui is starting...\")\n",
    "    process2 = subprocess.Popen(['open-webui', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    read_stream(process2.stdout, print)\n",
    "    process2.wait()\n",
    "    print(Fore.GREEN + \"open-webui task is running in the background\" + Style.RESET_ALL)\n",
    "\n",
    "def iframe_thread(port):\n",
    "    # 等待端口打开\n",
    "    while True:\n",
    "        time.sleep(0.5)\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        result = sock.connect_ex(('127.0.0.1', port))\n",
    "        if result == 0:\n",
    "            sock.close()\n",
    "            break\n",
    "        sock.close()\n",
    "\n",
    "    # 获取IP地址\n",
    "    try:\n",
    "        public_ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip()\n",
    "        ip_message = f\"{Fore.GREEN}\\nIP: {Fore.RED}{public_ip}{Style.RESET_ALL}\\n\"\n",
    "    except Exception as e:\n",
    "        ip_message = f\"{Fore.RED}\\nError fetching public IP: {str(e)}{Style.RESET_ALL}\\n\"\n",
    "    \n",
    "    print(ip_message)\n",
    "\n",
    "    # 启动隧道\n",
    "    p = subprocess.Popen([\"lt\", \"--port\", str(port)], stdout=subprocess.PIPE, text=True)\n",
    "    for line in p.stdout:\n",
    "        print(line.strip())\n",
    "\n",
    "# 在后台线程中运行 Ollama 任务\n",
    "ollama_thread = threading.Thread(target=run_ollama_task)\n",
    "ollama_thread.start()\n",
    "# 等待 Ollama 线程完成\n",
    "ollama_thread.join()\n",
    "threading.Thread(target=run_open_webui).start()\n",
    "\n",
    "threading.Thread(target=iframe_thread, daemon=True, args=(8080,)).start()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
